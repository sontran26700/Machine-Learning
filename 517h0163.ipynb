{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ngrams\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16382\n"
     ]
    }
   ],
   "source": [
    "filename = 'train.txt'\n",
    "with open(filename,encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "print(len(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_tokens_count= 321720\n",
      "V = 16681\n",
      "[(',', 19071), ('<s>', 16328), ('the', 13061), ('and', 9306), ('to', 8222), ('of', 7082), ('a', 6577), ('that', 5681), ('i', 5302), ('in', 4981)]\n"
     ]
    }
   ],
   "source": [
    "# tính 1-gram và tần suất\n",
    "# tính V (số lượng từ khác nhau)\n",
    "counter_unigram=Counter()\n",
    "all_tokens_count=0\n",
    "for line in lines:\n",
    "    tokens = nltk.word_tokenize(line.lower())\n",
    "    if len(tokens)==0:\n",
    "        continue \n",
    "    tokens = ['<s>'] + tokens\n",
    "    all_tokens_count+=len(tokens)\n",
    "    counter_unigram.update(tokens)\n",
    "    \n",
    "print('all_tokens_count=',all_tokens_count)\n",
    "V=len(counter_unigram)\n",
    "print('V =',V)\n",
    "print(counter_unigram.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111439\n",
      "[(',_and', 2308), ('<s>_and', 2169), ('of_the', 1363), ('in_the', 1357), (\"it_'s\", 1205), ('<s>_i', 1178), ('<s>_so', 1120), (',_i', 977), (',_the', 965), ('<s>_it', 885)]\n"
     ]
    }
   ],
   "source": [
    "# tính 2-gram và tần suất \n",
    "counter_bigram = Counter()\n",
    "for sent in lines:\n",
    "    tokens = nltk.word_tokenize(sent.lower())\n",
    "    tokens = ['<s>'] + tokens\n",
    "    gram2=ngrams(tokens,2)\n",
    "    join_grams = ['_'.join(list(gram)) for gram in gram2]\n",
    "    counter_bigram.update(join_grams)\n",
    "\n",
    "print(len(counter_bigram))\n",
    "#in thử một số \n",
    "print(counter_bigram.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210144\n",
      "[(\"<s>_it_'s\", 428), (',_and_i', 274), ('<s>_now_,', 257), ('<s>_and_i', 254), (\",_it_'s\", 223), ('<s>_thank_you', 203), ('<s>_this_is', 189), ('said_,_``', 169), (\"it_'s_a\", 167), ('a_lot_of', 155)]\n"
     ]
    }
   ],
   "source": [
    "# tính 3-gram và tần suất \n",
    "counter_trigram = Counter()\n",
    "for sent in lines:\n",
    "    tokens = nltk.word_tokenize(sent.lower())\n",
    "    tokens = ['<s>'] + tokens\n",
    "    gram3=ngrams(tokens,3)\n",
    "    join_grams = ['_'.join(list(gram)) for gram in gram3]\n",
    "    counter_trigram.update(join_grams)\n",
    "\n",
    "print(len(counter_trigram))\n",
    "#in thử một số \n",
    "print(counter_trigram.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tính theo Laplace \n",
    "anpha = 0.001\n",
    "def uni_prob(word):\n",
    "    return max(1,counter_unigram[word])/all_tokens_count\n",
    "\n",
    "def bi_prob(word1,word2):\n",
    "    bi_gram = '_'.join([word1,word2])\n",
    "    return (counter_bigram[bi_gram]+anpha)/(counter_unigram[word1]+V*anpha)  \n",
    "\n",
    "def tri_prob(word1,word2,word3):\n",
    "    bi_gram = '_'.join([word1,word2])\n",
    "    tri_gram = '_'.join([word1,word2,word3])\n",
    "    return (counter_trigram[tri_gram]+anpha)/(counter_bigram[bi_gram]+V*anpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backoff\n",
    "# tính prob theo từng loại: 1-gram, 2-gram, 3-gram\n",
    "def uni_prob(word):\n",
    "    return max(1,counter_unigram[word])/all_tokens_count\n",
    "\n",
    "def bi_prob(word1,word2):\n",
    "    bi_gram = '_'.join([word1,word2])\n",
    "    if counter_bigram[bi_gram]>0:\n",
    "        return counter_bigram[bi_gram]/counter_unigram[word1]\n",
    "    else:\n",
    "        return 0.4*uni_prob(word2)\n",
    "    \n",
    "def tri_prob(word1,word2,word3):\n",
    "    bi_gram = '_'.join([word1,word2])\n",
    "    tri_gram = '_'.join([word1,word2,word3])\n",
    "    if counter_trigram[tri_gram]>0:\n",
    "        return counter_trigram[tri_gram]/counter_bigram[bi_gram]\n",
    "    else:\n",
    "        return 0.4*bi_prob(word2,word3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laplace smoothing\n",
    "# tính xác suất của một câu, normalize theo 1 từ \n",
    "def probLM(sent,n):\n",
    "    if n>3 or n<1: # không xét trường hợp này \n",
    "        return 0\n",
    "    tokens=nltk.word_tokenize(sent.lower())\n",
    "    tokens = ['<s>']+tokens\n",
    "    prob=1\n",
    "    for i in range(1,len(tokens)):\n",
    "        if n==1:\n",
    "            prob*=uni_prob(tokens[i])\n",
    "        elif n==2:\n",
    "            prob*=bi_prob(tokens[i-1],tokens[i])\n",
    "        elif n==3:\n",
    "            if i>=2:\n",
    "                prob*=tri_prob(tokens[i-2],tokens[i-1],tokens[i])\n",
    "            else:\n",
    "                prob*=bi_prob(tokens[i-1],tokens[i])\n",
    "    size = len(tokens)-1\n",
    "    return prob**(1/size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=1\n",
      "prob= 0.0014228645509824066\n",
      "perplexity= 702.8075858025679\n",
      "n=2\n",
      "prob= 0.01602336610840924\n",
      "perplexity= 62.408859239332294\n",
      "n=3\n",
      "prob= 0.147231919138491\n",
      "perplexity= 6.792005469000022\n"
     ]
    }
   ],
   "source": [
    "# tính xác suất và Perplexity cho một câu Test \n",
    "# 1-gram\n",
    "# 2-gram\n",
    "# 3-gram \n",
    "sent='the human body with new abilities is no longer a question'\n",
    "#sent='A few years back I to to from'\n",
    "print('n=1')\n",
    "pr=probLM(sent,1)\n",
    "print('prob=',pr)\n",
    "print('perplexity=',1/pr)\n",
    "\n",
    "print('n=2')\n",
    "pr=probLM(sent,2)\n",
    "print('prob=',pr)\n",
    "print('perplexity=',1/pr)\n",
    "\n",
    "print('n=3')\n",
    "pr=probLM(sent,3)\n",
    "print('prob=',pr)\n",
    "print('perplexity=',1/pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob= 0.147231919138491\n",
      "perplexity= 6.792005469000022\n",
      "prob= 0.0036709318692291744\n",
      "perplexity= 272.410394859761\n"
     ]
    }
   ],
   "source": [
    "sent1='the human body with new abilities is no longer a question'\n",
    "sent2='the human body with knew abilities is know longer a question'\n",
    "pr=probLM(sent1,3)\n",
    "print('prob=',pr)\n",
    "print('perplexity=',1/pr)\n",
    "\n",
    "pr=probLM(sent2,3)\n",
    "print('prob=',pr)\n",
    "print('perplexity=',1/pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019884178609611593\n",
      "0.019884178609611593\n",
      "the human,,\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "def getWord():\n",
    "    word = []\n",
    "    filename = 'train.txt'\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        #print(lines)\n",
    "        \n",
    "    for sent in lines:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "    \n",
    "        for x in words:\n",
    "            word.append(x)\n",
    "\n",
    "    return word\n",
    "\n",
    "def preWord(text,n):\n",
    "    xr = ''\n",
    "    probTemp = 0\n",
    "    words =[]\n",
    "    words = getWord(); \n",
    "    while n > 0:\n",
    "        for x in words:\n",
    "            if probLM(text+x,1) > probTemp:\n",
    "                probTemp = probLM(text+x,2)\n",
    "                xr = x\n",
    "        text += xr\n",
    "    \n",
    "        print(probTemp)    \n",
    "                    #print(probLM(text+x,2)\n",
    "                    \n",
    "        n-=1\n",
    "    return text\n",
    "print(preWord(\"the human\",2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
