{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "import codecs\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='train.txt'\n",
    "filetest='test.txt'\n",
    "fd = codecs.open(filename,'r',encoding='utf-8')\n",
    "ft = codecs.open(filetest,'r',encoding='utf-8')\n",
    "data = fd.readlines()\n",
    "data1=ft.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "unigram_dict = {}\n",
    "#words = nltk.work_tokenize(data[0])\n",
    "#print(words)\n",
    "total_count = 0 \n",
    "bi_counter= Counter()\n",
    "#unigram = Counter()\n",
    "for sent in data:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    total_count += len(words)\n",
    "#     gram2=ngram(words,2)\n",
    "#     join_grams = ['_'.join(list(gram)) for gram in gram2]\n",
    "#     bi_counter.update(join_grams)\n",
    "    for w in words:\n",
    "        if w in unigram_dict.keys():\n",
    "            unigram_dict[w] +=1\n",
    "        else:\n",
    "            unigram_dict[w] =1\n",
    "            \n",
    "for sent in data1:\n",
    "    words = nltk.word_tokenize(sent)\n",
    "    total_count += len(words)\n",
    "    for a in words:\n",
    "        if a in unigram_dict.keys():\n",
    "            unigram_dict[a]+=1\n",
    "        else:\n",
    "            unigram_dict[a]=1\n",
    "            \n",
    "            \n",
    "#print(len()unigram_dict)\n",
    "#print(total_count)\n",
    "#print(total_count)\n",
    "#print(unigram_dict[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('P(think)=',unigram_dict['think']/total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_1gram 3.070702427242356e-29\n"
     ]
    }
   ],
   "source": [
    "input_sent=  ' The first lesson is: get ahead of the crisis'\n",
    "words = nltk.word_tokenize(input_sent)\n",
    "p = 1\n",
    "for w in words:\n",
    "    if w in unigram_dict.keys():\n",
    "        p *= unigram_dict[w]/total_count\n",
    "    else:\n",
    "        p = 0\n",
    "        break\n",
    "# gram_perlexity = 1/math.sqrt(p)\n",
    "# print(gram_perlexity)\n",
    "print('p_1gram',p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_bigram: 8.171109628936142e-19\n",
      "1106265786.946884\n"
     ]
    }
   ],
   "source": [
    "bi_counter1 = Counter()\n",
    "bi_counter2 = Counter()\n",
    "unigram = Counter()\n",
    "total_words = 0\n",
    "unigram_dic={}\n",
    "\n",
    "for sent in data:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        total_words += len(words)\n",
    "        gram1=ngrams(words, 1)\n",
    "        join_grams1 = ['_'.join(list(gram)) for gram in gram1]\n",
    "        unigram.update(join_grams1)\n",
    "        gram2=ngrams(words, 2)\n",
    "        join_grams = ['_'.join(list(gram)) for gram in gram2]\n",
    "        bi_counter1.update(join_grams)\n",
    "        total = sum(bi_counter1.values())\n",
    "        \n",
    "#print(total)\n",
    "        \n",
    "for sent in data1:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        total_words += len(words)\n",
    "        gram1=ngrams(words, 1)\n",
    "        join_grams1 = ['_'.join(list(gram)) for gram in gram1]\n",
    "        unigram.update(join_grams1)\n",
    "        gram2=ngrams(words, 2)\n",
    "        join_grams = ['_'.join(list(gram)) for gram in gram2]\n",
    "        bi_counter2.update(join_grams)\n",
    "        \n",
    "input_sent = ' The first lesson is: get ahead of the crisis'\n",
    "words = nltk.word_tokenize(input_sent)\n",
    "p=1\n",
    "#print(words)\n",
    "\n",
    "for i in range(len(words)):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        else:\n",
    "            bigram=words[i-1]+'_'+words[i]\n",
    "            unigram_count = unigram[words[i-1]]\n",
    "            \n",
    "        if bigram in bi_counter1.keys():\n",
    "            p *= bi_counter1[bigram]/unigram_count\n",
    "        else:\n",
    "            p = 0\n",
    "            break\n",
    "            \n",
    "#print(unigram_count)            \n",
    "print('P_bigram:', p)\n",
    "\n",
    "\n",
    "# for x in bi_counter1:\n",
    "#     if x in bi_counter2:\n",
    "#         if x in unigram_dict.keys():\n",
    "#             pass\n",
    "#         else:\n",
    "#             unigram_dic[x]=bi_counter1[x]\n",
    "# p=1           \n",
    "# for x in unigram_dic:\n",
    "#     p *= unigram_dic[x]/total\n",
    "\n",
    "#print(\"Probability : \" , p)\n",
    "\n",
    "#print(len(unigram_dic))\n",
    "\n",
    "\n",
    "\n",
    "gram_perlexity = math.sqrt(1/p)\n",
    "print(gram_perlexity)\n",
    "            \n",
    "#print('2gram:', p)\n",
    "#print(unigram_count[words[i-1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7057\n",
      "P_3gram: 9.344758140170687e-21\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "unigram = Counter()\n",
    "total_words = 0\n",
    "\n",
    "for sent in data:\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        total_words += len(words)\n",
    "        gram1=ngrams(words, 1)\n",
    "        join_grams1 = ['_'.join(list(gram)) for gram in gram1]\n",
    "        unigram.update(join_grams1)\n",
    "        gram2=ngrams(words, 3)\n",
    "        join_grams = ['_'.join(list(gram)) for gram in gram2]\n",
    "        counter.update(join_grams)\n",
    "        \n",
    "input_sent = ' The first lesson is: get ahead of the crisis'\n",
    "words = nltk.word_tokenize(input_sent)\n",
    "p=1\n",
    "#print(words)\n",
    "\n",
    "\n",
    "for i in range(len(words)):\n",
    "        if i <= 1:\n",
    "            continue\n",
    "        else:\n",
    "            bigram=words[i-2]+'_'+words[i-1]+'_'+words[i]\n",
    "            unigram_count = unigram[words[i-2]]\n",
    "            \n",
    "        if bigram in counter.keys():\n",
    "            p *= counter[bigram]/unigram_count\n",
    "        else:\n",
    "            p = 0\n",
    "            break\n",
    "            \n",
    "print(unigram_count)\n",
    "print('P_3gram:', p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
